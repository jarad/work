\documentclass{beamer}

\usepackage{verbatim,amsfonts,hyperref}
%\usefonttheme{serif}

\usecolortheme[RGB={0,0,144}]{structure}
\usetheme{AnnArbor}\usecolortheme{beaver}
\setbeamertemplate{navigation symbols}{}

\title{Bayesian Computation}
\subtitle{Inference via sampling}
\author{Jarad Niemi}
\institute[Iowa State]{Iowa State University}
\date{15 Nov 2011}


\setkeys{Gin}{width=0.6\textwidth}

%\renewcommand{\Gamma}{\mathrm\Gamma}

\begin{document}

%\section{???} \begin{comment}

\maketitle

\frame{\frametitle{\uncover<5->{Monte Carlo integration}}
\setkeys{Gin}{width=\textwidth}
	How can you evaluate the following?
	\[ \int_0^1 \theta^3(1-\theta)^6 d\theta \]
	\begin{columns}[c]
	\begin{column}{0.4\textwidth}
<<fig=TRUE,echo=FALSE>>=
set.seed(4)
# Integral to evaluate
f <- function(x,a=3,b=7) return(x*x^(a-1)*(1-x)^(b-1))
curve(f, cex.axis=2, cex.lab=1.5, lwd=2, xlab=expression(theta), 
          ylab=expression(f(theta)))
@
\end{column}\pause
\begin{column}{0.6\textwidth}
	\begin{enumerate}[<+->][1.]
	\item Analytically
	\item Reimann integration
	\item Lebesgue integration
	\item Monte Carlo integration
	\end{enumerate}
\end{column}
\end{columns}
}

\section{Monte Carlo integration}
\frame{\frametitle{Monte Carlo integration}
	To estimate $I=\int_a^b f(\theta)d\theta$\pause, do the following:
	\begin{itemize}[<+->]
	\item For $j=1,\ldots,J$, 
		\begin{itemize}
		\item Sample $\theta^{(j)}\sim Unif(a,b)$
		\item Calculate $f(\theta^{(j)})$
		\end{itemize}
	\item Approximate $I$ by 
	\[ I_J = (b-a) \frac{1}{J} \sum_{j=1}^J f(\theta^{(j)}).\]
	\end{itemize}
	\begin{center}
	\uncover<6->{
<<fig=TRUE,echo=FALSE>>=
par(mfrow=c(2,2),mar=c(5,5,4,2)+.1)
curve(f, cex.axis=2, cex.lab=1.5, lwd=2, xlab="", 
          ylab="", main="Reimann integration", axes=F, frame=T)
points(xx<-seq(0.05,1,by=.1),rep(0,10),pch='x')
segments(xx,0,xx,f(xx))
curve(f, cex.axis=2, cex.lab=1.5, lwd=2, xlab="", 
          ylab="", main="Monte Carlo integration", axes=F, frame=T)
points(xx<-runif(10),rep(0,10),pch='x')
segments(xx,0,xx,f(xx))
@
}\end{center}
}


\frame{\frametitle{}
\setkeys{Gin}{width=0.6\textwidth}
\begin{center}
<<mc-int,fig=TRUE,echo=FALSE>>=
# Monte Carlo integration
n <- 1e3
y <- f(runif(n), a=3, b=7)
par(mar=c(5,4,0,0)+.1)
plot(cumsum(y)/(1:n), type="l", ylim=c(.0011,.0013), xlab="Number of samples",
       ylab="Integral")
@
\end{center}
}

\subsection{Theory}
\frame{\frametitle{Theory}
	In trying to estimate $I = \int_a^b f(\theta) d\theta\pause=(b-a) \overline{f(\theta)}$\pause, we have 
	\begin{itemize}
	\item Strong Law of Large Numbers:\pause
	\[ I_J = (b-a) \frac{1}{J} \sum_{j=1}^J f\left(\theta^{(j)}\right) \quad \mathop{\longrightarrow}_{J\to\infty} \quad I \quad a.s.\]
	if $\theta^{(j)}\stackrel{iid}{\sim} Unif(a,b)$.
	
	\vspace{0.2in} \pause 
	
	\item Central Limit Theorem:
	\[ I_J \stackrel{d}{\to} N\left(I, (b-a)^2 \sigma^2/J\right) \]
	\pause where 
	\[ \hat{\sigma}^2=\frac{1}{J} \sum_{j=1}^J \left[f\left(\theta^{(j)}\right)-I_J\right]^2 \]
	\end{itemize}
}

\subsection{Recognizing pdfs}
\frame{\frametitle{}
	Rearrange
	\[ 
	\int_0^1 \theta^3(1-\theta)^6 d\theta 
	\pause = \alert<2>{\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}} \alert<3>{\int_0^1 \theta \alert<2>{\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}} \theta^{\alpha-1}(1-\theta)^{\beta-1} d\theta} 
	 \]
	 where $\alpha=3$ and $\beta=7$.
	 
	 \vspace{0.2in} \pause \pause
	 
	 So, 
	 \[ \int_0^1 \theta^3(1-\theta)^6 d\theta = c E[\theta] \]
	 where $c= \frac{\Gamma(3)\Gamma(7)}{\Gamma(3+7)}$ and $\theta\sim Be(3,7)$.
	 
	 \vspace{0.2in} \pause
	 
	 Suppose we didn't know $E[\theta]$ but could sample from $\theta\sim Be(3,7)$\pause, then 
	 
	 \[ c \frac{1}{J} \sum_{j=1}^J \theta^{(j)} \pause \quad\mathop{\longrightarrow}_{J\to\infty}\quad\int_0^1 \theta^3(1-\theta)^6 d\theta \quad a.s.  \]
	 \pause if $\theta^{(j)}\stackrel{iid}{\sim} Be(3,7)$.
}

\frame{\frametitle{}

\setkeys{Gin}{width=0.6\textwidth}
%\vspace{-0.4in}

\begin{center}
<<fig=TRUE, echo=FALSE>>=
<<mc-int>>
# Alternative Monte Carlo integration
c <- gamma(3)*gamma(7)/gamma(10)
y2 <- rbeta(n,3,7)*c
lines(cumsum(y2)/(1:n), col="red")
abline(h=3*c/10, col="seagreen")
legend("topright",c("Uniform","Beta","Truth"), lwd=1, col=c("black","red","seagreen"))
@
\end{center}
}


\section{Bayesian inference}
\subsection{Unknown normalizing constants}
\frame{\frametitle{Bayesian inference}
	In Bayesian statistics, our inferential goal is the posterior, e.g. 
	\[ p(\theta|y) \pause = \frac{p(y|\theta)p(\theta)}{p(y)} \pause \propto p(y|\theta)p(\theta) \qquad  \uncover<4->{p(y)= \int p(y|\theta)p(\theta) d\theta} \]
	where the model, $p(y|\theta)$, and prior, $p(\theta)$, are assumed. 
	
	\vspace{0.2in} \pause
	
	\begin{itemize}[<+->]
	\item $p(y)$ is analytically and numerically intractable
	\item $p(\theta|y)$ is a probability density (mass) function
%	\item Can evaluate
%		\begin{itemize}
%		\item $p(y|\theta)$
%		\item $p(\theta)$
%		\end{itemize}
	\item we can sample from $p(\theta|y)$
	\end{itemize}
}


\subsection{Exact sampling}
\frame{\frametitle{Exact sampling}
\setkeys{Gin}{width=\textwidth}
	Suppose $p(\theta|y) \propto \theta^{3-1}(1-\theta)^{7-1}\mathrm{I}(0<\theta<1)$, \pause  sample $\theta^{(j)}\stackrel{iid}{\sim} Be(3,7)$. \pause
	
<<exact,fig=TRUE,echo=FALSE>>=
x <- rbeta(n,3,7)
brks<-seq(0,1,.01)
par(mfrow=c(2,2))
hist(x,brks,freq=F, main="Exact sampling estimate", 
       xlab=expression(theta), ylab=expression(f(theta)))
#points(x,rep(0,n),pch='x')
curve(dbeta(x,3,7), col="seagreen",lwd=2, add=T)
legend("topright", c("Truth"), col="seagreen", lwd=2)
plot(x,type='l', xlab='j', ylab=expression(theta^j),ylim=c(0,1))
@
}


\subsection{Importance sampling}
\frame{\frametitle{Importance sampling}
\setkeys{Gin}{width=\textwidth}
	Suppose $p(\theta|y) \propto \theta^{3-1}(1-\theta)^{7-1}\mathrm{I}(0<\theta<1)$, \pause sample $\theta^{(j)}\stackrel{iid}{\sim} Unif(0,1)$ \pause and calculate $w_j\propto p(\theta^{(j)}|y)$. \pause
	
<<is,fig=TRUE,echo=FALSE>>=
require(weights)
x <- runif(n)
w <- dbeta(x,3,7); w <- w/sum(w)
par(mfrow=c(2,2))
wtd.hist(x,brks,weight=w,freq=F, main="Importance sampling estimate", 
       xlab=expression(theta), ylab=expression(f(theta)))
#points(x,rep(0,n),pch='x', col='red')
curve(dbeta(x,3,7), col="seagreen",lwd=2, add=T)
#sum(w*x)
plot(x,type='l',xlab='j', ylab=expression(theta^j),ylim=c(0,1))
@
}


\subsection{Metropolis}
\frame{\frametitle{Metropolis algorithm}
\setkeys{Gin}{width=0.4\textwidth}
	Suppose $\theta^{(j)}$ is current value\pause , simulate $\theta^*\sim N(\theta^{(j)},\sigma^2)$\pause, then set $\theta^{(j+1)}=\theta^*$ with probability
	\[ \min\left[1, \frac{p(\theta^*|y)}{p(\theta^{(j)}|y)} \right] \]
	\pause otherwise set $\theta^{(j+1)}=\theta^{(j)}$. \pause
	
<<metropolis,fig=TRUE,echo=FALSE>>=
par(mar=c(4,5,1,0)+.1)
curve(dbeta(x,3,7),col="seagreen",lwd=2, cex.lab=1.5, cex.axis=1.5, xlab=expression(theta), ylab=expression(f(theta)))
@
}

\frame{\frametitle{Metropolis}
\setkeys{Gin}{width=\textwidth}
<<fig=TRUE,echo=FALSE>>=
# Metropolis-Hastings
f <- function(x,a,b,log) dbeta(x,a,b,log=log)
mh <- function(xc, tune) {
  xp <- rnorm(1, xc, tune)
  la <- f(xp,3,7,log=T)-f(xc,3,7,log=T)
  if (log(runif(1))<la) return(xp) 
  return(xc)
}
x <- rep(NA,n); x[1] <- rbeta(1,3,7)
for (i in 2:n) x[i] <- mh(x[i-1],0.3)
par(mfrow=c(2,2))
hist(x,brks,freq=F, xlim=c(0,1), main="Metropolis sampling estimate", 
       xlab=expression(theta), ylab=expression(f(theta)))
curve(dbeta(x,3,7), col="seagreen",lwd=2, add=T)
plot(x,type='l', xlab='j', ylab=expression(theta^j),ylim=c(0,1))
@
}


\section{Gibbs sampling}
\frame{\frametitle{Two-stage Gibbs sampler}
	Everything is generalizable to sampling vector $\theta$ from $p(\theta|y)$\pause, but
	\begin{itemize}
	\item importance sampling and 
	\item Metropolis
	\end{itemize}
	proposal distributions are difficult to create.
	
	\vspace{0.2in} \pause
	
	Instead, break $\theta$ into two (or more) sub-vectors $\theta_1,\theta_2$ \pause and 
	\begin{itemize}[<+->]
	\item $\theta_1^{(j)}\sim p(\theta_1|\theta_2^{(j-1)},y)$
	\item $\theta_2^{(j)}\sim p(\theta_2|\theta_1^{(j)},y)$
	\end{itemize}
	
	\vspace{0.1in} \pause
	
	$\left(\theta_1^{(j)},\theta_2^{(j)}\right)$ are samples from $p(\theta|y)$.
}

\section{Summary}
\frame{\frametitle{Summary}
	Take home points:
	\begin{itemize}
	\item Bayesian inference involves integration
	\item Integration is solved via Monte Carlo methods
		\begin{itemize}
		\item Exact sampling
		\item Importance sampling
		\item Markov chain Monte Carlo 
			\begin{itemize}
			\item Metropolis-Hastings
			\item Gibbs sampling
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	If interested:
	\begin{itemize}
	\item Presentation made using Sweave (R+\LaTeX+beamer)
	\item Source posted on my website (\url{http://jaradniemi.com/presentations})
	\end{itemize}
}

\end{document}

